{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271ad386",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce750c86",
   "metadata": {},
   "source": [
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this context, EA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
    "\n",
    "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce327c",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Loading Data</a>\n",
    "\n",
    "<a href=#three>3. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#four>4. Data Engineering</a>\n",
    "\n",
    "<a href=#five>5. Modeling</a>\n",
    "\n",
    "<a href=#six>6. Model Performance</a>\n",
    "\n",
    "<a href=#seven>7. Model Explanations</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe958347",
   "metadata": {},
   "source": [
    " <a id=\"one\"></a>\n",
    "## 1. Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Importing Packages ⚡ |\n",
    "| :--------------------------- |\n",
    "| Required to import, and briefly discuss, the libraries that will be used throughout analysis and modelling. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2643d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4106347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.sklearn import log_model\n",
    "\n",
    "experiment = Experiment(\n",
    "  api_key=\"hbu6nFnbDgCuG2gKUAdO3w4D2\",\n",
    "  project_name=\"classification-sprint-team-jm5\",\n",
    "  workspace=\"gzukhanye-gmail-com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77deed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist, bigrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation\n",
    "import matplotlib.pyplot as plt  # For basic data visualization\n",
    "import seaborn as sns  # For enhanced data visualization\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Setting global constants to ensure notebook results are reproducible\n",
    "PARAMETER_CONSTANT = 42 ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d16df",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "## 2. Loading the Data\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Loading the data ⚡ |\n",
    "| :--------------------------- |\n",
    "| Load the data from the `train and test` file into a DataFrame. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86baf31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded the data from the df file as a dataFrame\n",
    "df_train = pd.read_csv(\"train.csv\") \n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b38162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7631be",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Exploratory data analysis ⚡ |\n",
    "| :--------------------------- |\n",
    "| Perform an in-depth analysis of all the variables in the DataFrame. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208bccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the number of rows and columns \n",
    "df_train.shape\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4603951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays info about the columns 'message' is categorical whereas 'tweetId' & 'sentiment' are numerical\n",
    "df_train.info()\n",
    "print(\"\\n\")\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The distribution of classes in 'sentiment'\n",
    "sns.countplot(x='sentiment', data=df_train)\n",
    "plt.title('Distribution of Sentiments')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542e261",
   "metadata": {},
   "source": [
    "Class Description\n",
    "\n",
    "2 News: the tweet links to factual news about climate change\n",
    "\n",
    "1 Pro: the tweet supports the belief of man-made climate change\n",
    "\n",
    "0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "\n",
    "-1 Anti: the tweet does not believe in man-made climate change Variable definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deab5ad",
   "metadata": {},
   "source": [
    "#### Tweet Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19358e63",
   "metadata": {},
   "source": [
    "1. Average length of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f80961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the characteristics of the tweet text\n",
    "df_train['tweet_length'] = df_train['message'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8c95f",
   "metadata": {},
   "source": [
    "2. Distribution of tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60427c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_train['tweet_length'], bins=50, kde=True)\n",
    "plt.title('Distribution of Tweet Lengths')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc570f",
   "metadata": {},
   "source": [
    "3. Common words and phrases\n",
    "\n",
    "Concatenates the all the tweets into a single string and tokenises the text into individual words and outputs the most common words & its frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(df_train['message'].astype(str))\n",
    "\n",
    "tokens = word_tokenize(all_text)\n",
    "fdist = FreqDist(tokens)\n",
    "common_words = fdist.most_common(10)\n",
    "print(\"Common Words:\", common_words)\n",
    "\n",
    "bi_grams = list(bigrams(tokens))\n",
    "bi_gram_freq = FreqDist(bi_grams)\n",
    "common_bigrams = bi_gram_freq.most_common(10)\n",
    "print(\"Common Bigrams:\", common_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d46b6",
   "metadata": {},
   "source": [
    "4. WordCloud\n",
    "\n",
    "WordCloud: data visualization technique used for representing text data\n",
    "in which the size of each word indicates its frequency or importance.\n",
    "widely used for analyzing data from social network websites. (Twitter)\n",
    "1 is the reference class(people who are in support of climate change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d36a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "positive_tweets = df_train[df_train['sentiment'] == 1]['message'].values\n",
    "positive_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(positive_tweets))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(positive_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('WordCloud for Positive Sentiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001e5fc",
   "metadata": {},
   "source": [
    "5. Verify for any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ee92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "Removing all the null values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function turns the sentiment codes into actual words Pro for 1, Anti for -1, Neutral for 0 and News for 2 which will make it easier for us to explore and visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentiment(df):\n",
    "    \"\"\"This function turns the sentiment codes into actual words which will make it easier for us to see\"\"\"\n",
    "\n",
    "    # creating an empty list for storage\n",
    "    sentiment_list = []\n",
    "\n",
    "    # Going through each sentiment and changing them accordingly, this is what this loop and the if statements do.\n",
    "    for number in df['sentiment'] :\n",
    "\n",
    "        if number == 1 :\n",
    "            sentiment_list.append('Pro')\n",
    "\n",
    "        elif number == 0 :\n",
    "            sentiment_list.append('Neutral')\n",
    "\n",
    "        elif number == -1 :\n",
    "            sentiment_list.append('Anti')\n",
    "\n",
    "        else :\n",
    "            sentiment_list.append('News')\n",
    "\n",
    "    # putting our sentiments in the column named 'sentiment' to our dataframe\n",
    "    df['sentiment'] = sentiment_list\n",
    "\n",
    "    return df\n",
    "clean_sentiment(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are extracting hashtags to know which sentiment likes which hashtag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_extract(tweet):\n",
    "    \"\"\"Helper function to extract hashtags\"\"\"\n",
    "    # creating a empty list for storage where we will keep our Hashtags later\n",
    "    hashtags = []\n",
    "\n",
    "    # Going through each tweet and looking for each hashtag and appending the Hashtags in our empty list hashtags\n",
    "    for i in tweet:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    # finding the sum of the elements in the list hashtag\n",
    "    hashtags = sum(hashtags, [])\n",
    "\n",
    "    # creating a dictionary with tokens from the list hashtags into a dictionary, where the keys are the frequency and the values is the frequency\n",
    "    frequency = nltk.FreqDist(hashtags)\n",
    "\n",
    "    # creating a dataframe from the dictionary to keep track of the word and the frequency\n",
    "    hashtag_df = pd.DataFrame({'hashtag': list(frequency.keys()),\n",
    "                           'count': list(frequency.values())})\n",
    "\n",
    "    # method is used to get n largest values from a dataframe\n",
    "    hashtag_df = hashtag_df.nlargest(15, columns=\"count\")\n",
    "\n",
    "    return hashtag_df\n",
    "\n",
    "def hashtag_extract(tweet):\n",
    "    \"\"\"Helper function to extract hashtags\"\"\"\n",
    "    # creating a empty list for storage where we will keep our Hashtags later\n",
    "    hashtags = []\n",
    "\n",
    "    # Going through each tweet and looking for each hashtag and appending the Hashtags in our empty list hashtags\n",
    "    for i in tweet:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "\n",
    "    # finding the sum of the elements in the list hashtag\n",
    "    hashtags = sum(hashtags, [])\n",
    "\n",
    "    # creating a dictionary with tokens from the list hashtags into a dictionary, where the keys are the frequency and the values is the frequency\n",
    "    frequency = nltk.FreqDist(hashtags)\n",
    "\n",
    "    # creating a dataframe from the dictionary to keep track of the word and the frequency\n",
    "    hashtag_df = pd.DataFrame({'hashtag': list(frequency.keys()),\n",
    "                           'count': list(frequency.values())})\n",
    "\n",
    "    # method is used to get n largest values from a dataframe\n",
    "    hashtag_df = hashtag_df.nlargest(15, columns=\"count\")\n",
    "\n",
    "    return hashtag_df\n",
    "\n",
    "\n",
    "#Extracting the hashtags for the pro sentiment tweets\n",
    "pro = hashtag_extract(df_train['message'][df_train['sentiment'] == 'Pro'])\n",
    "\n",
    "#Extracting the hashtags for the Anti sentiment tweets\n",
    "anti = hashtag_extract(df_train['message'][df_train['sentiment'] == 'Anti'])\n",
    "\n",
    "#Extracting the hashtags for the Neutral sentiment tweets\n",
    "neutral = hashtag_extract(df_train['message'][df_train['sentiment'] == 'Neutral'])\n",
    "\n",
    "#Extracting the hashtags for the News sentiment tweets\n",
    "news = hashtag_extract(df_train['message'][df_train['sentiment'] == \"News\"])\n",
    "\n",
    "\n",
    "#creating a dataframe with all the hashtags and a count for each sentiment\n",
    "df_hashtags = pro.merge(anti,on='hashtag',suffixes=('_pro', '_anti'), how = 'outer').merge(neutral,on='hashtag', how = 'outer').merge(news,on='hashtag', suffixes = ('_neutral', '_news'), how = 'outer')\n",
    "df_hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate joined words based on capitals\n",
    "def camel_case_split(identifier):\n",
    "\n",
    "    matches = re.finditer(\n",
    "        r'.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)',\n",
    "        identifier\n",
    "    )\n",
    "    return \" \".join([m.group(0) for m in matches])\n",
    "\n",
    "# Extract Mentions\n",
    "def extract_mentions(tweet):\n",
    "\n",
    "  \"\"\"Helper function to extract mentions\"\"\"\n",
    "  mentions = re.findall(r'@([a-zA-Z0-9_]{1}[a-zA-Z0-9_]{0,14})', tweet)\n",
    "\n",
    "  return mentions\n",
    "\n",
    "#Applying the function on the dataframe\n",
    "df_train['mentions'] = df_train['message'].apply(extract_mentions)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all the mentions\n",
    "mentions_list = [item for new_list in df_train['mentions'] for item in new_list]\n",
    "\n",
    "# Grouping mentions by sentiment\n",
    "# News Mentions\n",
    "news_mentions = df_train[df_train['sentiment'] == 'News']['mentions']\n",
    "news_mentions = [x for x in news_mentions if x != []]\n",
    "news_mentions = [item for new_list in news_mentions for item in new_list]\n",
    "\n",
    "# Positive Mentions\n",
    "pos_mentions = df_train[df_train['sentiment'] == 'Pro']['mentions']\n",
    "pos_mentions = [x for x in pos_mentions if x != []]\n",
    "pos_mentions = [item for new_list in pos_mentions for item in new_list]\n",
    "\n",
    "# Neutral Mentions\n",
    "neutral_mentions =df_train[df_train['sentiment'] == 'Neutral']['mentions']\n",
    "neutral_mentions = [x for x in neutral_mentions if x != []]\n",
    "neutral_mentions = [item for new_list in neutral_mentions for item in new_list]\n",
    "\n",
    "# Negative Mentions\n",
    "neg_mentions = df_train[df_train['sentiment'] == 'Anti']['mentions']\n",
    "neg_mentions = [x for x in neg_mentions if x != []]\n",
    "neg_mentions = [item for new_list in neg_mentions for item in new_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing RT from tweets\n",
    "df_train['message'] = df_train['message'].str.strip('rt ')\n",
    "\n",
    "df_test['message'] = df_test['message'].str.strip('rt ')\n",
    "\n",
    "# Remove @ mentions\n",
    "pattern = r\"@[\\w]+\" # pattern to remove\n",
    "\n",
    "pattern = r\"@[\\w]+\" # pattern to remove\n",
    "\n",
    "sub = r'' # replace it with with an empty space\n",
    "\n",
    "#transforming our dataframe\n",
    "df_train['message'] = df_train['message'].replace(to_replace = pattern, value = sub, regex = True)\n",
    "\n",
    "#transforming our dataframe\n",
    "df_test['message'] = df_test['message'].replace(to_replace = pattern, value = sub, regex = True)\n",
    "\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to lookup shortwords from the dictionary\n",
    "def lookup_dict(text, dictionary):\n",
    "\n",
    "    for word in text.split():\n",
    "\n",
    "        if word.lower() in dictionary:\n",
    "\n",
    "            if word.lower() in text.split():\n",
    "\n",
    "                text = text.replace(word, dictionary[word.lower()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary of contractions as the keys and its full word representation as the values\n",
    "short_and_contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"121\": \"one to one\",\n",
    "\"a/s/l\": \"age, sex, location\",\n",
    "\"adn\": \"any day now\",\n",
    "\"afaik\": \"as far as I know\",\n",
    "\"afk\": \"away from keyboard\",\n",
    "\"aight\": \"alright\",\n",
    "\"alol\": \"actually laughing out loud\",\n",
    "\"b4\": \"before\",\n",
    "\"b4n\": \"bye for now\",\n",
    "\"bak\": \"back at the keyboard\",\n",
    "\"bf\": \"boyfriend\",\n",
    "\"bff\": \"best friends forever\",\n",
    "\"bfn\": \"bye for now\",\n",
    "\"bg\": \"big grin\",\n",
    "\"bta\": \"but then again\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cid\": \"crying in disgrace\",\n",
    "\"cnp\": \"continued in my next post\",\n",
    "\"cp\": \"chat post\",\n",
    "\"cu\": \"see you\",\n",
    "\"cul\": \"see you later\",\n",
    "\"cul8r\": \"see you later\",\n",
    "\"cya\": \"bye\",\n",
    "\"cyo\": \"see you online\",\n",
    "\"dbau\": \"doing business as usual\",\n",
    "\"fud\": \"fear, uncertainty, and doubt\",\n",
    "\"fwiw\": \"for what it's worth\",\n",
    "\"fyi\": \"for your information\",\n",
    "\"g\": \"grin\",\n",
    "\"g2g\": \"got to go\",\n",
    "\"ga\": \"go ahead\",\n",
    "\"gal\": \"get a life\",\n",
    "\"gf\": \"girlfriend\",\n",
    "\"gfn\": \"gone for now\",\n",
    "\"gmbo\": \"giggling my butt off\",\n",
    "\"gmta\": \"great minds think alike\",\n",
    "\"h8\": \"hate\",\n",
    "\"hagn\": \"have a good night\",\n",
    "\"hdop\": \"help delete online predators\",\n",
    "\"hhis\": \"hanging head in shame\",\n",
    "\"iac\": \"in any case\",\n",
    "\"ianal\": \"I am not a lawyer\",\n",
    "\"ic\": \"I see\",\n",
    "\"idk\": \"I don't know\",\n",
    "\"imao\": \"in my arrogant opinion\",\n",
    "\"imnsho\": \"in my not so humble opinion\",\n",
    "\"imo\": \"in my opinion\",\n",
    "\"iow\": \"in other words\",\n",
    "\"ipn\": \"I’m posting naked\",\n",
    "\"irl\": \"in real life\",\n",
    "\"jk\": \"just kidding\",\n",
    "\"l8r\": \"later\",\n",
    "\"ld\": \"later, dude\",\n",
    "\"ldr\": \"long distance relationship\",\n",
    "\"llta\": \"lots and lots of thunderous applause\",\n",
    "\"lmao\": \"laugh my ass off\",\n",
    "\"lmirl\": \"let's meet in real life\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "\"ltr\": \"longterm relationship\",\n",
    "\"lulab\": \"love you like a brother\",\n",
    "\"lulas\": \"love you like a sister\",\n",
    "\"luv\": \"love\",\n",
    "\"m/f\": \"male or female\",\n",
    "\"m8\": \"mate\",\n",
    "\"milf\": \"mother I would like to fuck\",\n",
    "\"oll\": \"online love\",\n",
    "\"omg\": \"oh my god\",\n",
    "\"otoh\": \"on the other hand\",\n",
    "\"pir\": \"parent in room\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\",\n",
    "\"rofl\": \"roll on the floor laughing\",\n",
    "\"rpg\": \"role playing games\",\n",
    "\"ru\": \"are you\",\n",
    "\"shid\": \"slaps head in disgust\",\n",
    "\"somy\": \"sick of me yet\",\n",
    "\"sot\": \"short of time\",\n",
    "\"thanx\": \"thanks\",\n",
    "\"thx\": \"thanks\",\n",
    "\"ttyl\": \"talk to you later\",\n",
    "\"u\": \"you\",\n",
    "\"ur\": \"you are\",\n",
    "\"uw\": \"you’re welcome\",\n",
    "\"wb\": \"welcome back\",\n",
    "\"wfm\": \"works for me\",\n",
    "\"wibni\": \"wouldn't it be nice if\",\n",
    "\"wtf\": \"what the fuck\",\n",
    "\"wtg\": \"way to go\",\n",
    "\"wtgp\": \"want to go private\",\n",
    "\"ym\": \"young man\",\n",
    "\"gr8\": \"great\",\n",
    "\"8yo\":\"eight year old\",\n",
    "\"brb\" : \"be right back\"\n",
    "}\n",
    "\n",
    "#Apply a lambda function to look up every word in the tweets and replace it with the full word\n",
    "\n",
    "#apply this to a new column so that we can see the difference\n",
    "df_train['clean_message'] = df_train['message'].apply(lambda x: lookup_dict(x,short_and_contractions))\n",
    "df_test['clean_message'] = df_test['message'].apply(lambda x: lookup_dict(x,short_and_contractions))\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"This function is to clean the data removing urls, punctuations, spaces, making text lowercase, \n",
    "    removing 'rt', removing standalone numbers,\n",
    "    and removing single consonant letters that don't make sense to be alone.\n",
    "    \"\"\"\n",
    "\n",
    "    URL = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "    text = re.sub(URL, '', text)\n",
    "\n",
    "    text = text.lower()  # Making text lowercase\n",
    "\n",
    "    text = re.sub(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\", \"\", text)  # Removing all punctuation with no space\n",
    "\n",
    "    text = re.sub(\"(<br\\s/><br\\s/?)|(-)|(_)|(/)|(:).\", \" \", text)  # Removing all punctuation with a space\n",
    "\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \"\", text)  # Removing standalone numbers\n",
    "\n",
    "    text = re.sub(\"\\\\s+\", \" \", text)  # Remove extra whitespace\n",
    "\n",
    "    text = re.sub(r\"U+FFFD \", ' ', text)  # Remove that funny diamond\n",
    "\n",
    "    text = text.lstrip()  # Removes whitespaces before the string\n",
    "\n",
    "    text = text.rstrip()  # Removes whitespaces after the string\n",
    "\n",
    "    text = re.sub(r'\\brt\\b', '', text)  # Removing the word 'rt' (assuming it's a separate word)\n",
    "\n",
    "    text = re.sub(r'\\b[bcdfghjklmnpqrstvwxyz]\\b', '', text)  # Removing single consonant letters\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "df_train['clean_message'] = df_train['clean_message'].apply(clean_text)\n",
    "\n",
    "df_test['clean_message'] = df_test['clean_message'].apply(clean_text)\n",
    "\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations on our training dataset\n",
    "df_train['clean_message'] = df_train['clean_message'].apply(lambda x: ''.join([l for l in x if l not in string.punctuation]))\n",
    "\n",
    "# removing punctuations on our testing dataset\n",
    "df_test['clean_message'] = df_test['clean_message'].apply(lambda x: ''.join([l for l in x if l not in string.punctuation]))\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start by tokenizing the tweets\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "\n",
    "# transforming the data using the Treebankword tokenizer\n",
    "df_train['tokenized'] = df_train['clean_message'].apply(tokeniser.tokenize)\n",
    "\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising our lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# applying lemmatization to the POS column\n",
    "df_train['lemmatized'] = df_train['tokenized'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# joining the tokenised words after they have been lemmatized\n",
    "df_train['lemmatized'] = [' '.join(map(str, l)) for l in df_train['lemmatized']]\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Removing words that has no relevance to the context (https, RT, CO)\n",
    "df_train['word_cloud'] = df_train['lemmatized'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "# Removing common words which appear in all sentiments\n",
    "remove_words = ['climate', 'change', 'rt', 'global', 'warming', 'donald', 'trump','amp', 'realDonaldTrump', 's','aaa']\n",
    "\n",
    "# Function to remove common words listed above\n",
    "def remove_common_words(message):\n",
    "  pattern = re.compile(r'\\b(' + r'|'.join(remove_words) + r')\\b\\s*')\n",
    "  message = pattern.sub('', message)\n",
    "  return message\n",
    "\n",
    "df_train['word_cloud'] = df_train['word_cloud'].apply(remove_common_words)\n",
    "\n",
    "\n",
    "# Adding select words to stop words for better analysis on important word frequency\n",
    "stop = set(stopwords.words('english'))\n",
    "stop_words = [\"via\", \"co\", \"I\",'We','The'] + list(stop)\n",
    "\n",
    "# Removing stop words from the tweets\n",
    "df_train['word'] = df_train['word_cloud'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n",
    "df_train['word'] = df_train['word'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "# Separating the strings to a list of words\n",
    "word_list = [word for line in df_train['word'] for word in line.split()]\n",
    "\n",
    "# Creating a word frequency counter\n",
    "sns.set(style=\"darkgrid\")\n",
    "counts = Counter(word_list).most_common(15)\n",
    "counts_df = pd.DataFrame(counts)\n",
    "counts_df\n",
    "counts_df.columns = ['word', 'frequency']\n",
    "\n",
    "# Creating a word frequency plot\n",
    "fig, ax = plt.subplots(figsize = (9, 9))\n",
    "ax = sns.barplot(y=\"word\", x='frequency', ax = ax, data=counts_df, palette=\"hls\")\n",
    "plt.title('WORD FREQUENCY')\n",
    "#plt.savefig('wordcount_bar.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Average length of tweets\n",
    "\n",
    "\n",
    "It is interesting to look at the length of the tweets for each sentiment, which sentiment writes the longest or the shortest tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Number of Words per Tweet from the lemmatized words\n",
    "df_train[\"num_words\"] = df_train[\"lemmatized\"].apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "\n",
    "# plotting boxplot for the length of how long each tweet is per sentiment\n",
    "sns.boxplot(x=df_train['sentiment'], y=df_train['lemmatized'].str.len(), data=df_train, palette=(\"rainbow\"), ax=ax)\n",
    "\n",
    "# title of the boxplot\n",
    "plt.title('Length of characters in each tweet per sentiment')\n",
    "\n",
    "# showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Distribution of tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "\n",
    "# plotting boxplot for the length of how long each tweet is per sentiment\n",
    "sns.boxplot(x='sentiment', y='num_words', data=df_train, palette=(\"hls\"))\n",
    "\n",
    "# title of the boxplot\n",
    "plt.title('Number of words in each tweet for each sentiment')\n",
    "\n",
    "# showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hashtag analysis\n",
    "\n",
    "\n",
    "Now lets look at the hashtags that were used in the tweets. This will give us an indication of which hashtags each sentiment frequently uses. This could possibly help us tell whether a tweet is Pro or Anti climate change.\n",
    "\n",
    "We previously extracted the hashtags before cleaning the 'message column', we will now use that dataframe of hashtags in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2,\n",
    "                         nrows=2,\n",
    "                         figsize=(20, 15))\n",
    "\n",
    "sns.barplot(data=pro,y=pro['hashtag'], x=pro['count'], ax=axes[0,0]).set(title = 'Pro climate change hashtags')\n",
    "\n",
    "sns.barplot(data=anti,y=anti['hashtag'], x=anti['count'], ax=axes[0,1]).set(title = 'Anti climate change hashtags')\n",
    "\n",
    "sns.barplot(data=neutral,y=neutral['hashtag'], x=neutral['count'], ax=axes[1,0]).set(title = 'Neutral climate change hashtags')\n",
    "\n",
    "sns.barplot(data=news,y=news['hashtag'], x=news['count'], ax=axes[1,1]).set(title = 'News climate change hashtags')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Common words and phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting word clouds\n",
    "news = df_train[df_train['sentiment'] == 'News']['lemmatized']\n",
    "pro = df_train[df_train['sentiment'] == 'Pro']['lemmatized']\n",
    "neutral =df_train[df_train['sentiment'] == 'Neutral']['lemmatized']\n",
    "anti = df_train[df_train['sentiment'] == 'Anti']['lemmatized']\n",
    "\n",
    "\n",
    "news = [word for line in news for word in line.split()]\n",
    "pro = [word for line in pro for word in line.split()]\n",
    "neutral = [word for line in neutral for word in line.split()]\n",
    "anti= [word for line in anti for word in line.split()]\n",
    "\n",
    "news = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=100,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(news))\n",
    "\n",
    "pro = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=100,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(pro))\n",
    "\n",
    "\n",
    "\n",
    "neutral = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=100,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(neutral))\n",
    "\n",
    "\n",
    "anti = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=100,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(anti))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (20, 12))\n",
    "fig.tight_layout(pad = 0)\n",
    "\n",
    "axs[0, 0].imshow(news)\n",
    "axs[0, 0].set_title('Frequent words from news climate tweets', fontsize = 20)\n",
    "axs[0, 0].axis('off')\n",
    "\n",
    "axs[0, 1].imshow(pro)\n",
    "axs[0, 1].set_title('Frequent words from pro climate tweets', fontsize = 20)\n",
    "axs[0, 1].axis('off')\n",
    "\n",
    "\n",
    "axs[1, 0].imshow(anti)\n",
    "axs[1, 0].set_title('Frequent words from anti climate tweets', fontsize = 20)\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "axs[1, 1].imshow(neutral)\n",
    "axs[1, 1].set_title('Frequent words from neutral climate tweets', fontsize = 20)\n",
    "axs[1, 1].axis('off')\n",
    "#plt.savefig('joint_cloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Bigram Analyis for each Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "fig.suptitle('Bigrams in Tweets')\n",
    "stopwords = set(STOPWORDS)\n",
    "more_stopwords = {'https', 'https rt', 'rt'}  # Remove 'rt' from stopwords\n",
    "stopwords = stopwords.union(more_stopwords)\n",
    "\n",
    "# Create subplots for different sentiment categories\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Plot for the Pro sentiment of the bigrams\n",
    "plt.subplot(2, 2, 1)\n",
    "bigram_d = list(bigrams([w for w in word_tokenize(' '.join(df_train.loc[df_train.sentiment == 'Pro', 'clean_message']).lower()) if\n",
    "                        (w not in stopwords) & (w.isalpha())]))\n",
    "d_fq = FreqDist(bg for bg in bigram_d)\n",
    "bgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\n",
    "bgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\n",
    "bgdf_d = bgdf_d.sort_values('count', ascending=False)\n",
    "sns.barplot(x=bgdf_d.head(10)['count'], y=bgdf_d.index[:10], color='pink')\n",
    "plt.title('Pro Tweets')\n",
    "\n",
    "# Plot for the News sentiment of the bigrams\n",
    "plt.subplot(2, 2, 2)\n",
    "bigram_nd = list(bigrams([w for w in word_tokenize(' '.join(df_train.loc[df_train.sentiment == 'News', 'clean_message']).lower()) if\n",
    "                        (w not in stopwords) & (w.isalpha())]))\n",
    "nd_fq = FreqDist(bg for bg in bigram_nd)\n",
    "bgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n",
    "bgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\n",
    "bgdf_nd = bgdf_nd.sort_values('count', ascending=False)\n",
    "sns.barplot(x=bgdf_nd.head(10)['count'], y=bgdf_nd.index[:10], color='b')\n",
    "plt.title('News Tweets')\n",
    "\n",
    "# Plot for the Anti sentiment of the bigrams\n",
    "plt.subplot(2, 2, 3)\n",
    "bigram_nd = list(bigrams([w for w in word_tokenize(' '.join(df_train.loc[df_train.sentiment == 'Anti', 'clean_message']).lower()) if\n",
    "                        (w not in stopwords) & (w.isalpha())]))\n",
    "nd_fq = FreqDist(bg for bg in bigram_nd)\n",
    "bgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n",
    "bgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\n",
    "bgdf_nd = bgdf_nd.sort_values('count', ascending=False)\n",
    "sns.barplot(x=bgdf_nd.head(10)['count'], y=bgdf_nd.index[:10], color='c')\n",
    "plt.title('Anti Tweets')\n",
    "\n",
    "# Plot for the Neutral sentiment of the bigrams\n",
    "plt.subplot(2, 2, 4)\n",
    "bigram_nd = list(bigrams([w for w in word_tokenize(' '.join(df_train.loc[df_train.sentiment == 'Neutral', 'clean_message']).lower()) if\n",
    "                        (w not in stopwords) & (w.isalpha())]))\n",
    "nd_fq = FreqDist(bg for bg in bigram_nd)\n",
    "bgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n",
    "bgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\n",
    "bgdf_nd = bgdf_nd.sort_values('count', ascending=False)\n",
    "sns.barplot(x=bgdf_nd.head(10)['count'], y=bgdf_nd.index[:10], color='g')\n",
    "plt.title('Neutral Tweets')\n",
    "\n",
    "plt.tight_layout()  # To ensure proper spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of stop words\n",
    "fig.suptitle('Trigrams in Tweets')\n",
    "stopwords = set(STOPWORDS)\n",
    "more_stopwords = {'https', 'https rt', 'rt'}  # Remove 'rt' from stopwords\n",
    "stopwords = stopwords.union(more_stopwords)\n",
    "\n",
    "# Create subplots for different sentiment categories\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Function to extract trigrams from text\n",
    "def extract_trigrams(text):\n",
    "    tokens = [w for w in word_tokenize(text.lower()) if (w not in stopwords) and (w.isalpha())]\n",
    "    trigram = list(ngrams(tokens, 3))\n",
    "    return trigram\n",
    "\n",
    "# Plot for the Pro sentiment of the trigrams\n",
    "plt.subplot(2, 2, 1)\n",
    "trigrams_d = extract_trigrams(' '.join(df_train.loc[df_train.sentiment == 'Pro', 'clean_message']))\n",
    "d_fq = FreqDist(trigram for trigram in trigrams_d)\n",
    "trigram_df_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\n",
    "trigram_df_d.index = trigram_df_d.index.map(lambda x: ' '.join(x))\n",
    "trigram_df_d = trigram_df_d.sort_values('count', ascending=False)\n",
    "sns.barplot(x=trigram_df_d.head(10)['count'], y=trigram_df_d.index[:10], color='pink')\n",
    "plt.title('Pro Tweets')\n",
    "\n",
    "# Plot for the News sentiment of the trigrams\n",
    "plt.subplot(2, 2, 2)\n",
    "trigrams_nd = extract_trigrams(' '.join(df_train.loc[df_train.sentiment == 'News', 'clean_message']))\n",
    "nd_fq = FreqDist(trigram for trigram in trigrams_nd)\n",
    "trigram_df_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n",
    "trigram_df_nd.index = trigram_df_nd.index.map(lambda x: ' '.join(x))\n",
    "trigram_df_nd = trigram_df_nd.sort_values('count', ascending=False)\n",
    "sns.barplot(x=trigram_df_nd.head(10)['count'], y=trigram_df_nd.index[:10], color='b')\n",
    "plt.title('News Tweets')\n",
    "\n",
    "# Plot for the Anti sentiment of the trigrams\n",
    "plt.subplot(2, 2, 3)\n",
    "trigrams_a = extract_trigrams(' '.join(df_train.loc[df_train.sentiment == 'Anti', 'clean_message']))\n",
    "nd_fq = FreqDist(trigram for trigram in trigrams_a)\n",
    "trigram_df_a = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\n",
    "trigram_df_a.index = trigram_df_a.index.map(lambda x: ' '.join(x))\n",
    "trigram_df_a = trigram_df_a.sort_values('count', ascending=False)\n",
    "sns.barplot(x=trigram_df_a.head(10)['count'], y=trigram_df_a.index[:10], color='c')\n",
    "plt.title('Anti Tweets')\n",
    "\n",
    "# Plot for the Neutral sentiment of the trigrams\n",
    "plt.subplot(2, 2, 4)\n",
    "trigrams_n = extract_trigrams(' '.join(df_train.loc[df_train.sentiment == 'Neutral', 'clean_message']))\n",
    "nd_fq = FreqDist(trigram for trigram in trigrams_n)\n",
    "trigram_df_n = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n",
    "trigram_df_n.index = trigram_df_n.index.map(lambda x: ' '.join(x))\n",
    "trigram_df_n = trigram_df_n.sort_values('count', ascending=False)\n",
    "sns.barplot(x=trigram_df_n.head(10)['count'], y=trigram_df_n.index[:10], color='g')\n",
    "plt.title('Neutral Tweets')\n",
    "\n",
    "plt.tight_layout()  # To ensure proper spacing between subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dec933f0",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "## 4. Data Engineering\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Data engineering ⚡ |\n",
    "| :--------------------------- |\n",
    "| Clean the dataset, and possibly create new features - as identified in the EDA phase. |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_train, df_test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a55924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def preprocessing(text):\n",
    "    # Remove mentions (@user), URLs, and special characters\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove special characters\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in text.split() if word.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered words back into a sentence\n",
    "    cleaned_message = ' '.join(filtered_text)\n",
    "\n",
    "    return cleaned_message\n",
    "\n",
    "# Apply the preprocessing function to the 'message' column\n",
    "df_combined['processed_message'] = df_combined['message'].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f1e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "# Apply the tokenize function to the 'processed_message' column\n",
    "df_combined['tokens'] = df_combined['processed_message'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemma\n",
    "def lemmatize(tokens):\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Apply the lemmatize function to the 'tokens' column\n",
    "df_combined['lemma'] = df_combined['tokens'].apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_combined.iloc[:len(df_train)]\n",
    "df_test = df_combined.iloc[len(df_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c605ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_count(words, word_dict={}):\n",
    "    \"\"\" this function takes in a list of words and returns a dictionary \n",
    "        with each word as a key, and the value represents the number of \n",
    "        times that word appeared\"\"\"\n",
    "    words = df_train['clean_message']\n",
    "    for word in words:\n",
    "        if word in word_dict.keys():\n",
    "            word_dict[word] += 1\n",
    "        else:\n",
    "            word_dict[word] = 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5785e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df_train['clean_message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58b0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_message = {}\n",
    "\n",
    "# Grouping by 'clean_message'\n",
    "grouped_df = df_train.groupby('clean_message')\n",
    "\n",
    "# Iterating over unique messages\n",
    "for message, group in grouped_df:\n",
    "    clean_message[message] = {}\n",
    "\n",
    "    # Iterating over rows in the group\n",
    "    for row in group['clean_message']:\n",
    "        clean_message[message] = bag_of_words_count(row, clean_message[message])\n",
    "\n",
    "# Now clean_message should contain bag-of-words representations for each unique message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1076f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "\n",
    "for pp in words:\n",
    "    # Check if the key exists in the dictionary\n",
    "    if pp in clean_message:\n",
    "        for word in clean_message[pp]:\n",
    "            all_words.add(word)\n",
    "\n",
    "# Assuming 'all' is a key in the dictionary\n",
    "total_words = sum([v for v in clean_message.get('all', {}).values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming clean_message is a dictionary with word frequencies for different categories\n",
    "clean_message['all'] = Counter()\n",
    "\n",
    "for pp in words:\n",
    "    for word, frequency in clean_message[pp].items():\n",
    "        clean_message['all'][word] += frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d1499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming clean_message is a dictionary with word frequencies for a specific category, like 'all'\n",
    "word_frequencies = clean_message['all'].values()\n",
    "\n",
    "plt.hist([v for v in word_frequencies if v < 10], bins=10, density=False)\n",
    "plt.ylabel(\"# of words\")\n",
    "plt.xlabel(\"word frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f13559",
   "metadata": {},
   "source": [
    "The histogram above displays that there are no words that appear in the tweets less than 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bbfefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Assuming 'clean_message' is a column in your DataFrame\n",
    "words = df_train['clean_message'].str.split().explode()\n",
    "\n",
    "# Count occurrences of each word\n",
    "word_counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rare words\n",
    "rare_words = [word for word, count in word_counts.items() if count < 2000]\n",
    "\n",
    "print(rare_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4178766",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "## 5. Modelling\n",
    "<a class=\"anchor\" id=\"1.1\"></a>\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "---\n",
    "    \n",
    "| ⚡ Description: Modelling ⚡ |\n",
    "| :--------------------------- |\n",
    "| Create one or more classification models that are able to accurately predict |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e530b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert processed tokens to text\n",
    "df_train['lemma'] = df_train['lemma'].apply(lambda tokens: ' '.join(tokens))\n",
    "df_test['lemma'] = df_test['lemma'].apply(lambda tokens: ' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['lemma'], df_train['sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4514ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=50000)  # You can adjust the number of features as needed\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ce93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM classifier\n",
    "svm_classifier = SVC(kernel='rbf', random_state=42)\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_test is your test DataFrame with 'tweetid' and 'lemma' columns\n",
    "X_test_tfidf = tfidf_vectorizer.transform(df_test['lemma'])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Create a DataFrame for submission\n",
    "df_submission = pd.DataFrame({'tweetid': df_test['tweetid'], 'sentiment': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_submission.to_csv('svm_submission.csv', index=False)\n",
    "\n",
    "# Optionally, print accuracy and classification report\n",
    "print(\"Number of Rows:\", len(df_submission))\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_test['tweetid'] has 10564 rows\n",
    "# Use the correct DataFrame for the number of rows\n",
    "df_submission = pd.DataFrame({'tweetid': df_test['tweetid'].iloc[:len(predictions)], 'sentiment': predictions})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_submission.to_csv('svm_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and preprocess text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_pad = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_pad = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = 75  # Choose an appropriate value\n",
    "X_train_pad = pad_sequences(X_train_pad, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_pad, maxlen=max_len, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_len))\n",
    "model.add(Conv1D(256, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "# Use a lower learning rate and add a learning rate scheduler\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with more epochs\n",
    "model.fit(X_train_pad, y_train_encoded, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test_encoded)\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train_encoded)\n",
    "y_test_onehot = to_categorical(y_test_encoded)\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_len))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pad, y_train_onehot, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test_onehot)\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0b86cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "knn_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = knn_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f9368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(random_state=42)\n",
    "random_forest_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = random_forest_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f50c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svm_accuracy =  0.7345132743362832  \n",
    "cnn_accuracy =   0.7051200866699219\n",
    "cnn_two_accuracy =  0.7082806825637817\n",
    "\n",
    "\n",
    "models = ['SVM', 'CNN', 'CNN_TWO']\n",
    "\n",
    "accuracies = [svm_accuracy, cnn_accuracy,cnn_two_accuracy ]\n",
    "\n",
    "plt.bar(models, accuracies, color=['green', 'red', 'orange'])\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
